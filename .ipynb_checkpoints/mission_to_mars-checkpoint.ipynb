{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import pymongo\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "executable_path = {'executable_path': 'chromedriver.exe'} # Ed needs this to run on his PC\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Step 1 - Scrape the Sites for info\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1-1 - Scrape the NASA Mars News Site for the latest News Title and Paragraph Text\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open browser to scrape the Nasa Mars News Site and collect the latet News Title and Paragraph Text. \n",
    "# URL = https://mars.nasa.gov/news/\n",
    "browser.visit('https://mars.nasa.gov/news/')\n",
    "\n",
    "# pause for brower to open\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in the HTML from the page\n",
    "html = browser.html\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# print(soup.prettify()) # if need to see what was pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan and pull the content_title div\n",
    "news_title_list = soup.find_all('div', class_='content_title')\n",
    "\n",
    "# Pull the text from the second item, \"1\" which is the title\n",
    "news_title = news_title_list[1].text\n",
    "# print(news_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan and pull the article_teaser_body div\n",
    "news_paragraph_list = soup.find_all('div', class_='article_teaser_body')\n",
    "\n",
    "# Pull the text from the first item, \"1\" which is the short paragraph\n",
    "news_p = news_paragraph_list[0].text\n",
    "# print(news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 1-2 - Scrape the JPL Mars Space Images for Featured Image\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open browser to scrape the JPL Site and collect the items and images \n",
    "# URL = https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\n",
    "browser.visit('https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars')\n",
    "\n",
    "# pause for brower to open\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in the HTML from the page\n",
    "html = browser.html\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# print(soup.prettify()) # if need to see what was pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan and pull the first SearchResultCard div\n",
    "jpl_small_images_list = soup.find('div', class_='SearchResultCard')\n",
    "\n",
    "# Pull the 'href' item which is what must be 'clicked' to get to next page\n",
    "jpl_small_image = jpl_small_images_list.a['href']\n",
    "# print(jpl_small_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edwar\\anaconda3\\envs\\PythonData\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py:477: FutureWarning: browser.find_link_by_href is deprecated. Use browser.links.find_by_href instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Call browser to 'click' on the small image to get to the large one\n",
    "browser.click_link_by_href(jpl_small_image)\n",
    "\n",
    "# pause for brower to open\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in the HTML from the secondary page\n",
    "html = browser.html\n",
    "\n",
    "# Parse this new HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# print(soup.prettify()) # if need to see what was pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan and pull the BaseImagePlaceholder div\n",
    "jpl_large_images_list = soup.find_all('div', class_='BaseImagePlaceholder')\n",
    "# print(jpl_large_images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the url from the img data-src attribute for 1st \"0\" item\n",
    "featured_image_url = jpl_large_images_list[0].img[\"data-src\"]\n",
    "# print(featured_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 1-3 - Scrape the Mars Facts page for the Mars Facts Table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to scrape the Space-Facts.com site for the data table on mars \n",
    "# url = \"https://space-facts.com/mars/\"\n",
    "url = \"https://space-facts.com/mars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Pandas method to pull all the tables from the page\n",
    "tables = pd.read_html(url)\n",
    "# tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Mars table is the first \"0\" table on the page...\n",
    "mars_table = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per the requrested format, rename the columns...\n",
    "mars_table.rename(columns={0:\"Description\",1:\"Mars\"},inplace= True)\n",
    "\n",
    "# and rename the index\n",
    "mars_table.set_index(\"Description\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the StringIO function to put into a string\n",
    "str_io = io.StringIO()\n",
    "\n",
    "mars_table.to_html(buf=str_io, classes='table')\n",
    "\n",
    "mars_table_html_string = str_io.getvalue()\n",
    "# print(mars_table_html_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 1-4 - Scrape the Astrogeology site for High resolution Mars hempsphere pictures (4)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the Astrogeology Site and collect the items and images \n",
    "# breaking up the given url into 2 pieces since just the top portion is needed later\n",
    "# Complete URL is: https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\n",
    "# Break URL into pieces and combine for first search\n",
    "base_url = 'https://astrogeology.usgs.gov/'\n",
    "search_adder = 'search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "# browse the url\n",
    "browser.visit(base_url + search_adder) \n",
    "\n",
    "# pause for brower to open\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in the HTML from the page\n",
    "html = browser.html\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# print(soup.prettify()) # if need to see what was pulle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all the image items\n",
    "image_items_list = soup.find_all('div', class_='item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the list of images to click to the full image and pull in the url\n",
    "\n",
    "# create list to hold the image dictionary entries\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "for item in image_items_list:\n",
    "    # get the title, but need to strip off \"enhanced\" from the end\n",
    "    h_title_long = item.div.a.h3.text\n",
    "    h_title = h_title_long.rsplit(' ', 1)[0]\n",
    "    # print(h_title)\n",
    "    \n",
    "    # get the partial link, which needs the base_url added\n",
    "    h_link_end = item.a['href']\n",
    "    # print(h_link)\n",
    "    \n",
    "    # create the full link and go there with the browser\n",
    "    browser.visit(base_url + h_link_end) \n",
    "    #  Pause for it to render\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Pull in the HTML from the page that has the full .jpg\n",
    "    html = browser.html\n",
    "    \n",
    "    # Parse HTML with Beautiful Soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # find the image item\n",
    "    mars_large_image = soup.find('div', class_='wide-image-wrapper')\n",
    "    \n",
    "    # pull the href for the full image\n",
    "    mars_large_image_info = mars_large_image.div.ul.li.a[\"href\"]\n",
    "    \n",
    "    # append the image and name to the list of urls for hemisphere images \n",
    "    hemisphere_image_urls.append({\"title\":h_title,\"img_url\":mars_large_image_info})\n",
    "\n",
    "# print(hemisphere_image_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 1-5 - Clean up browser and any other housekeeping\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Part 1-6 - Create dictionary and store the scraped data into the dictionary to pass back when this is put into python script as function scrape\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'news_title': \"NASA's InSight Detects Two Sizable Quakes on Mars\", 'news_p': 'The magnitude 3.3 and 3.1 temblors originated in a region called Cerberus Fossae, further supporting the idea that this location is seismically active.', 'featured_image_url': 'https://d2pn8kiwq2w21t.cloudfront.net/images/jpegPIA24515.width-1024.jpg', 'mars_table_html_string': '<table border=\"1\" class=\"dataframe table\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>Mars</th>\\n    </tr>\\n    <tr>\\n      <th>Description</th>\\n      <th></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>Equatorial Diameter:</th>\\n      <td>6,792 km</td>\\n    </tr>\\n    <tr>\\n      <th>Polar Diameter:</th>\\n      <td>6,752 km</td>\\n    </tr>\\n    <tr>\\n      <th>Mass:</th>\\n      <td>6.39 × 10^23 kg (0.11 Earths)</td>\\n    </tr>\\n    <tr>\\n      <th>Moons:</th>\\n      <td>2 (Phobos &amp; Deimos)</td>\\n    </tr>\\n    <tr>\\n      <th>Orbit Distance:</th>\\n      <td>227,943,824 km (1.38 AU)</td>\\n    </tr>\\n    <tr>\\n      <th>Orbit Period:</th>\\n      <td>687 days (1.9 years)</td>\\n    </tr>\\n    <tr>\\n      <th>Surface Temperature:</th>\\n      <td>-87 to -5 °C</td>\\n    </tr>\\n    <tr>\\n      <th>First Record:</th>\\n      <td>2nd millennium BC</td>\\n    </tr>\\n    <tr>\\n      <th>Recorded By:</th>\\n      <td>Egyptian astronomers</td>\\n    </tr>\\n  </tbody>\\n</table>', 'hemisphere_image_urls': [{'title': 'Cerberus Hemisphere', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'}, {'title': 'Schiaparelli Hemisphere', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'}, {'title': 'Syrtis Major Hemisphere', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'}, {'title': 'Valles Marineris Hemisphere', 'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]}\n"
     ]
    }
   ],
   "source": [
    "# set up dictionary to pass values back to calling program when this is an app\n",
    "mars_dictionary = []\n",
    "mars_dictionary = {\n",
    "    'news_title': news_title,\n",
    "    'news_p': news_p,\n",
    "    'featured_image_url': featured_image_url,\n",
    "    'mars_table_html_string': mars_table_html_string,\n",
    "    'hemisphere_image_urls' : hemisphere_image_urls    \n",
    "}\n",
    "print(mars_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Addendum - connect to Mongo and insert dictionary into Mongo for testing with HTML\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MongoDB\n",
    "#Adds data scraped above into MongoDB\n",
    "# conn = 'mongodb://localhost:27017'\n",
    "conn = 'mongodb+srv://edober:M)ng002!2o@dscluster.cgqvg.mongodb.net/myFirstDatabase?retryWrites=true&w=majority'\n",
    "client = pymongo.MongoClient(conn)\n",
    "db = client['mars_app']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edwar\\anaconda3\\envs\\PythonData\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: update is deprecated. Use replace_one, update_one or update_many instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n': 1,\n",
       " 'nModified': 0,\n",
       " 'upserted': ObjectId('60679aee2509e052bd59ff50'),\n",
       " 'opTime': {'ts': Timestamp(1617402606, 2), 't': 2},\n",
       " 'electionId': ObjectId('7fffffff0000000000000002'),\n",
       " 'ok': 1.0,\n",
       " '$clusterTime': {'clusterTime': Timestamp(1617402606, 2),\n",
       "  'signature': {'hash': b'Z\\xcc\\\\O\\xab\\xfe\\xda\\xf1\\xd2\\xea,0d\\x13\\xdd\\xc0d\\xe8X*',\n",
       "   'keyId': 6946601059972284418}},\n",
       " 'operationTime': Timestamp(1617402606, 2),\n",
       " 'updatedExisting': False}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.collection.update({}, mars_dictionary, upsert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
